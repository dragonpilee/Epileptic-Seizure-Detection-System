# -*- coding: utf-8 -*-
"""Epileptic_seizure_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M3K1xlXZS4wTeva941Rp0fjVvRwDXpbC

# Epileptic Seizure Detection with Wavelet Transform

#### Sandra Anna Joshy
Epileptic seizure is a disease that affects a large number of people and it is caused by abnormal electrical activity in the brain [1]. Most seizures are very brief and rarely life threatening but critical seizures have to be recognized immediately.Also, according to studies[6], it is possible to control the attacks with early diagnosis.In addition Epileptic seizure patients require special treatments and continuous observation for the disease symptoms. Therefore several methods for epileptic seizure recognition have been developed in the past for early diagnosis. One of them is recognition with EEG signals. Even though EEG signals are a strong tool for epileptic seizure recognition, visual inspect for discriminating EEGs is a time consuming and high costly process [2].Therefore this project is aimed to make epileptic seizure detection using EEG signals that are transformed by wavelet transform, with machine learning and deep learning techniques.

## Table of Content

[Problem](#problem)   
[Data Understanding](#data_understanding)   
[Data Preparation](#data_preparation)   
[Modeling](#modeling)   
[Evaluation](#evaluation)   
[References](#references)

## Problem <a class="anchor" id="problem"></a>

Epilepsy is a group of neurological disorders characterized by recurrent epileptic seizures.[3]This seizures can cause severe shaking attacks therefore in these seizures patients can hurt her/hisself very badly.Luckily epilepsy is a curable diasease that 70% of cases are controlable with early diagnosis and medication[4][5].Therefore in order to make an early diagnosis, many epilepsy diagnostic methods have been developed over time. One of them is diagnosis with the help of EEG signals.However the visual inspect process for discriminating EEGs is a time consuming and high costly process.Therefore the project is aimed to make epileptic seizure detection very quickly and accurately by using EEG signal of the brain data with machine learning and deep learning techniques.

## Data Understanding<a class="anchor" id="data_understanding"></a>

For this task,I will use Bonn University’s restructured EEG time series dataset that consists of 5 different target classes , 179 attributes and it contains 11500 samples.The classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure. Only subjects in class 1 have epileptic seizure.I provied more details about dataset below.

![alt text](https://drive.google.com/uc?id=1nkOyWgqc1jc7LXD-aOfzOoA_cQa7eJ0p)

$$$$

![alt text](https://drive.google.com/uc?id=1UrYrwvOzx0w57axwOJL7FvJ16M-d2V8B)

detection
"""

from google.colab import files
uploaded = files.upload() #choose data.csv file from provided files

import pandas as pd
import numpy as np
df=pd.read_csv("data.csv")
df.head()

print("General info about colums,rows etc.")
df.info()
print("\nTarget variables value counts\n",df["y"].value_counts())

import matplotlib.pyplot as plt
def hist(df,plt):
  plt.hist(df[df["y"]==1]["y"],label="epileptic seizure activity")
  plt.hist(df[df["y"]!=1]["y"],label="not a seizure")
  plt.legend(loc='lower right')
  plt.show()

hist(df,plt)

"""All class have same amount of instances and dataset looks balance however i will transform target variable into seizure or not so dataset will no longer be balanced.

## Data Preparation<a class="anchor" id="data_preparation"></a>

In data preparation process, firstly i cleaned the data from unnecessary columns then I transformed the target variable into binary classes (1 for seizure, 0 for not seizure).Then I created new variables with some mathematical transformations that called Hurst exponent and Discrete wavelet transformation (do not worry, I will explain all these transformations below.). Then i normalize data.After i restructured the unbalanced data into more balanced data.Lastly I inspected whether feature selection improve accuracies.

######**Cleaning the data and transforming the target variable**
"""

df.head(2) #just a quick look to data again. x1...x178 columns are a part of our time-series data (EEG Signals) but Unnamed: 0 column must be inspected.

df["Unnamed: 0"].value_counts #As you can see this column is exlusive for all instance
                              #So it means, the column has no effect on classification, it is unnecessary

df["y"].value_counts() # I will transform 2,3,4,5 classes to 0, 1 class to 1

#This method drop the unnecessary column (Unnamed: 0) and transform the target variable
def prepareData(df):
  df["y"]=[1 if df["y"][i]==1 else 0 for i in range(len(df["y"]))]
  target=df["y"]
  df_copy=df.drop(["Unnamed: 0","y"],axis=1)
  return df_copy,target

df_copy,target=prepareData(df)

"""######**Data transformation with hurst exponent and discrete wavelet transform**

Before i started coding the project, I had some literature review.According to my review, I decided on not using raw time series data but using transformed data.I created new features with these transformations called hurst exponent and discrete wavelet transformation.$$$$

**Hurst Exponent :**
The Hurst exponent is used as a measure of long-term memory of time series. It relates to the autocorrelations of the time series, and the rate at which these decrease as the lag between pairs of values increases[7].Basically the main reason why I use this variable is that I think there may be certain fluctuation differences between the classes of time series data.$$$$

Hurst exponent formula:

![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/d32a26032869f8c902eb7a3062309dd88de63b5b)$$$$
**Discrete Wavelet Transform :**

Discrete Wavelet Transform (DWT) is a kind of transform that any wavelet transform for which the wavelets are discretely sampled[8].(Not an open explanation right? What is a wavelet transform anyway?).

**Note:** I used db4 type DWT which is very popular in literature, for this task and also provided a table that shows which types are used in literature.(see
figure 2)

**Wavelet Transform :**

Actually if you already familiar with Fourier Transform, Wavelet transform can be explained more concretely.In Fourier Transform, the transform can understand which frequencies (frequency domain) exist in the signal without any time information(time domain) but Wavelet transform can give more information about both time and frequencies domains.Wavelet transform give more information on frequency domain on small frequencies.On the other hand it give more information about time domain in large frequency values[9].


![alt text](https://drive.google.com/uc?id=1dUhbFebAHnD7h_Yq5GvyKGXKinrzPFXo)

(Picture is modified version of given link picture taken from [10]]

(For more detail about wavelet transform, you can also check [9] reference.)


$$$$

![alt text](https://drive.google.com/uc?id=1Mgf76Fem5ts2Tntx4GL-oYGVxc-HQBFy)

(Table 2, This table taken from [13] research paper)
"""

pip install hurst #installing hurst module to get hurst values

import pywt #importing pywt for getting wavelet transform features
from hurst import compute_Hc

def getHurst(df_copy):
  df_copy["hurst_ex"]=[compute_Hc(df_copy.iloc[i], kind="change", simplified=True)[0] for i in range(len(df_copy))]
  df_copy["hurst_c"]=[compute_Hc(df_copy.iloc[i], kind="change", simplified=True)[1] for i in range(len(df_copy))]
  return df_copy


def getStatsForHurst(df_copy):
  plt.scatter(df_copy["hurst_ex"],target)
  print("mean value of hurst exponent for class 1:",np.mean(df_copy.iloc[target[target==1].index]["hurst_ex"]))
  print("mean value of hurst exponent for class 0:",np.mean(df_copy.iloc[target[target==0].index]["hurst_ex"]))
  print("mean value of hurst constant for class 1:",np.mean(df_copy.iloc[target[target==1].index]["hurst_c"]))
  print("mean value of hurst constant for class 0:",np.mean(df_copy.iloc[target[target==0].index]["hurst_c"]))
  print("median value of hurst exponent for class 1:",np.median(df_copy.iloc[target[target==1].index]["hurst_ex"]))
  print("median value of hurst exponent for class 0:",np.median(df_copy.iloc[target[target==0].index]["hurst_ex"]))
  print("median value of hurst constant for class 1:",np.median(df_copy.iloc[target[target==1].index]["hurst_c"]))
  print("median value of hurst constant for class 0:",np.median(df_copy.iloc[target[target==0].index]["hurst_c"]))

#These methods create a new dataset with wavelet transform
#In getWaveletFeatures method, i get a group of wavelet coeffient and hurst exponent and the constant for all instance
#give these values to statisticsForWavelet function to get coeffients quartiles,mean,median,standart deviation,variance,root mean square and some other values.
#Lastly createDfWavelet method give all these values and return a new dataframe
def getWaveletFeatures(data,target):
    list_features = []
    for signal in range(len(data)):
        list_coeff = pywt.wavedec(data.iloc[signal], "db4")
        features = []
        features.append(data.iloc[signal]["hurst_ex"])
        features.append(data.iloc[signal]["hurst_c"])
        for coeff in list_coeff:
            features += statisticsForWavelet(coeff)
        list_features.append(features)
    return createDfWavelet(list_features,target)
#This method taken from [9]
def statisticsForWavelet(coefs):
    n5 = np.nanpercentile(coefs, 5)
    n25 = np.nanpercentile(coefs, 25)
    n75 = np.nanpercentile(coefs, 75)
    n95 = np.nanpercentile(coefs, 95)
    median = np.nanpercentile(coefs, 50)
    mean = np.nanmean(coefs)
    std = np.nanstd(coefs)
    var = np.nanvar(coefs)
    rms = np.nanmean(np.sqrt(coefs**2))
    return [n5, n25, n75, n95, median, mean, std, var, rms]

def createDfWavelet(data,target):
  for i in range(len(data)):
    data[i].append(target[i])
  return pd.DataFrame(data)

df_copy=getHurst(df_copy)
getStatsForHurst(df_copy)

df_copy_fea=getWaveletFeatures(df_copy,target)

df_copy_fea.head()#our new dataset is ready

"""**Resampling data**

Before the resampling, the 0 class was too prominent.I tried to balance the data by shuffling and resampling the data, In order get a more balanced dataset for machine learning models.
"""

from sklearn.utils import shuffle
def createBalancedDataset(data,random_state):
  #shuffling for random sampling
  X = shuffle(data,random_state=random_state)
  #getting first 6500 value
  return X.sort_values(by=47, ascending=False).iloc[:6500].index

v=createBalancedDataset(df_copy_fea,42)

plt.hist((df_copy_fea.iloc[v])[47])
(df_copy_fea.iloc[v][47]).value_counts() #more balanced dataset

"""**Normalizing Data**

Before the feature selection I normalized data.Normalizing data is necessary for Anova test.Also normalizing is a prerequisite for a lot of machine learning algorithms[11].
"""

#normalizing dataset
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(df_copy_fea.drop([47],axis=1))
n_df_fea=pd.DataFrame(scaler.transform(df_copy_fea.drop([47],axis=1)))

"""**Feature Selection with Anova and Random Forest**

Before giving inputs to models.I inspected whether feature selection improve the accuracies.In order to understand this situation, first i gave all features to model and then i gave only the selected features as input to model and i compared the accuries and f-scores.

For feature selection, I got help by below diagram.Since the dataset's input values are numeric but target variable is categoric, I choosed the Anova test for feature selection.Furthermore i did one more feature selection with random forest in order to compare them together.Eventhough feature selection can be very helpful on improving accuracies in many case, In this situation, it did not help so I decided to continue with all features as input.
"""

#First the all features are given as a input to SVM model (I will explain why i used SVM later)

import numpy as np
from sklearn.model_selection import train_test_split
X_trainr, X_testr, y_trainr, y_testr = train_test_split(n_df_fea.iloc[v], target.iloc[v], test_size=0.33, random_state=42)

#Reference point for svm
from sklearn import svm
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

#I will explain this model in model part in the notebook
clf = svm.SVC(kernel="linear")
clf.fit(X_trainr, y_trainr)
#cross validation is 10
y_pred = cross_val_predict(clf,X_testr,y_testr,cv=10)
print("All features are inclueded\n",classification_report(y_testr, y_pred))

"""**Selecting most important 20 features with Anova**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
#Selection most important 20 feature by using Anova test
def selectFeature(X_trainr,y_trainr,X_testr):
  sel_f = SelectKBest(f_classif, k=20)
  X_train_f = sel_f.fit_transform(X_trainr, y_trainr)
  mySelectedFeatures=[i for i in range(len(sel_f.get_support())) if sel_f.get_support()[i]==True]
  j=0
  unseable_columns=[]
  #Creating a new dataset with these 20 features
  for i in X_trainr.columns:
    if(j not in mySelectedFeatures):
      unseable_columns.append(i)
    j+=1
  X_train_arranged=X_trainr.drop(columns=unseable_columns)
  X_test_arranged=X_testr.drop(columns=unseable_columns)
  return  X_train_arranged,X_test_arranged

X_train_arranged,X_test_arranged=selectFeature(X_trainr,y_trainr,X_testr)

X_train_arranged.columns #The most important columns according to Anova

#Overall accuracy is decreased
from sklearn import svm
from sklearn.metrics import classification_report
clf = svm.SVC(kernel="linear")
clf.fit(X_train_arranged, y_trainr)
y_pred = cross_val_predict(clf,X_test_arranged,y_testr,cv=10)
print("Only Anova test's Features are used\n",classification_report(y_testr, y_pred))

"""**Selecting most important 20 features with Random Forest**"""

#Firstly I used grid Search for getting best hyperparameter for random-forest
np.random.seed(42)
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True)

param_grid = {
    'max_depth': [2,5],
    'min_samples_split':[2,5,10],
    'n_estimators': [100,150],
    'max_features': ['sqrt', 'log2']
}

CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)
CV_rfc.fit(X_trainr, y_trainr)
print (CV_rfc.best_params_)

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=42,max_depth=5,max_features='sqrt',min_samples_split=2,n_estimators=150)
clf.fit(X_trainr, y_trainr)
#I also get the importance rates and sort in a desending order and create a dataframe
zipped=pd.DataFrame(zip(X_trainr.columns,clf.feature_importances_),columns=["column","importance"]).sort_values(by="importance", ascending=False)
y_pred2 = cross_val_predict(clf,X_testr,y_testr,cv=10)
print("All featuares are included\n",classification_report(y_testr, y_pred2))

zipped.head(20) #The most important 20 values according to random forest

#F-scores are decreased
clf = svm.SVC(kernel="linear")
clf.fit(X_trainr[zipped.iloc[:20].index], y_trainr)
y_pred = cross_val_predict(clf,X_testr[zipped.iloc[:20].index],y_testr,cv=10)
print("Only random forest's features are inclueded\n",classification_report(y_testr, y_pred))

"""![alt text](https://drive.google.com/uc?id=1VwWDqamqvC8_W0CHLsWfKYLmhJVmPmKM)

As you can see, The feature selection process did not increase accuracies.Therefore I decided to continue with all features as input

## Modeling<a class="anchor" id="modeling"></a>

In the model selection process, I analysed the most used algoriths in the literature.I encounter that the most used algorithms are ANNs/RNNs, SVMs and Desion Trees.Therefore I decided to use SVM since this task is a binary classification problem , Random Forest for desion trees are pretty popular in the literature and lastly LSTM for RNN's are pretty popular in the literature and it is one of my favorites.
"""

#SVM, for kernel, I used some kernels and get the most accurate one
clf = svm.SVC(kernel="linear",probability=True)
clf.fit(X_trainr, y_trainr)
#cross validation is 10
y_pred = cross_val_predict(clf,X_testr,y_testr,cv=10)
print("All features are included\n",classification_report(y_testr, y_pred))

#Random forest, I got hyperparameters from above grid-search
clf1 = RandomForestClassifier(random_state=42, max_depth=5, max_features='sqrt', min_samples_split=5, n_estimators=150)
clf1.fit(X_trainr, y_trainr)
y_pred2 = cross_val_predict(clf1,X_testr,y_testr,cv=10)
print("All featuares are included\n",classification_report(y_testr, y_pred2))

#LSTM
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import LSTM
# When i designed the network, Basically i used heuristic approach
X_trainrr=np.array(X_trainr).reshape(X_trainr.shape[0],X_trainr.shape[1],1)
X_testrr=np.array(X_testr).reshape(X_testr.shape[0],X_testr.shape[1],1)
model = Sequential()
model.add(LSTM(50, input_shape=(X_trainrr.shape[1], X_trainrr.shape[2])))
model.add(Dropout(0.1))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
# fit network
history = model.fit(X_trainrr, y_trainr, epochs=50, batch_size=72, validation_data=(X_testrr, y_testr), verbose=2, shuffle=False)
# plot history
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

"""## Evaluation<a class="anchor" id="evaluation"></a>

For evalaute the model, I used f-scores, ROC curves and overall accuracies.I provided the models accuracies and f-scores as table below.I also provided ROC curves for SVM and Random Forest.

![alt text](https://drive.google.com/uc?id=1Z53mCM5oPJ-yFUI6T1g647KUPrfCTozf)

As you see the most accurate algorithm is Random Forest and all algorithms are more successful at classifiying 0 class than 1 class.

**ROC Curves**
"""

pip install scikit-plot

import scikitplot as skplt
import matplotlib.pyplot as plt
#ROC Curve for SVM
y_pred_proba=clf.predict_proba(X_testr)
skplt.metrics.plot_roc_curve(y_testr,y_pred_proba)
plt.show()

#ROC curve for Random-Forest
y_pred_proba=clf1.predict_proba(X_testr)
skplt.metrics.plot_roc_curve(y_testr,y_pred_proba)
plt.show()

"""As you can see both models ROC curve areas,f-scores of both classes and overall accuracies are very close to 1 and %100.Therefore the models are very accurate.

## References<a class="anchor" id="references"></a>

$$$$
[1] 	S. A. Hosseini, M-R. Akbarzadeh-T,M-B. Naghibi-Sistani, «Qualitative and Quantitative Evaluation of EEG,» MECS, 2013.

[2] 	Ling Guo, Daniel Rivero, Julián Dorado, Juan R. Rabunal, Alejandro Pazos, «Automatic epileptic seizure detection in EEGs based on line length feature and,» Journal of Neuroscience Methods, 2010.

[3]Chang BS, Lowenstein DH. Epilepsy. N Engl J Med. 2003;349(13):1257‐1266. doi:10.1056/NEJMra022308

[4]Eadie MJ. Shortcomings in the current treatment of epilepsy. Expert Rev Neurother. 2012;12(12):1419‐1427. doi:10.1586/ern.12.129

[5]D. D. A. ÖZKARDEŞ. [Online]. Available: https://www.memorial.com.tr/saglik-rehberleri/epilepsi-ile-ilgili-bilmeniz-gereken-10-gercek/.

[6]Yuan, Q., Li, F., & Zhong, H. (2015). Early diagnosis, treatment and prognosis of epilepsy with continuous spikes and waves during slow sleep. International journal of clinical and experimental medicine, 8(3), 4052–4058.

[7]«Hurst Exponent,» [Online]. Available: https://en.wikipedia.org/wiki/Hurst_exponent.

[8]«Discrete Wavelet Transform,» [Online]. Available: https://en.wikipedia.org/wiki/Discrete_wavelet_transform.

[9]A. Taşpınar, «A guide for using the Wavelet Transform in Machine Learning,» [Online]. Available: http://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/.

[10][Online]. Available: http://ataspinar.com/wp-content/uploads/2018/08/Comparisonoftransformations.jpg.

[11]A. Chavakula. [Online]. Available: https://www.quora.com/Which-machine-algorithms-require-data-scaling-normalization.

[12]J. Brownlee, «Machine Learning Mastery,» [Online]. Available: https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/11/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png.

**Disclaimer!** <font color='grey'>This notebook was prepared by Sandra Anna Joshy . The notebook is available for educational purposes only. There is no guarantee on the correctness of the content provided as it is a student work.

</font>
"""